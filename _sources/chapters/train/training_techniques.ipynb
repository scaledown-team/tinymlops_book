{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aff6a892",
   "metadata": {},
   "source": [
    "# Techniques for Training TinyML Models\n",
    "\n",
    "Previously we looked at some of the challenges of training TinyML models. In short, we need to train our models while recognizing the constraints of the hardware we are deploying to and the overall systems requirements like latency and accuracy.\n",
    "\n",
    "As engineers, if we train our models while being *aware* of our final hardware limitations, then we can take steps to reduce our model's performance drop when we deploy our model. Most of our model's performance will drop at two steps in our TinyML pipeline: when we optimize our model and when we deploy our model.\n",
    "\n",
    "Optimization reduces either the size or the number of operations in our model and it is the main place where our model's performance can drop. Deploying can reduce our model's performance due to constraints that are mostly out of our control like the size and speed of the memory, the presence of floating point units and the types of ops supported. However we can take into account these hardware constraints when we build our model architecture to make our system more efficient.\n",
    "\n",
    "## The Look-Ahead Method\n",
    "I like to call this technique of being aware of our entire pipeline the \"Look-Ahead\" method. Being able to look-ahead when building our system is an extremely powerful tool. However it is not possible to do it if we have not at least planned for and finallized the broad requirements and steps in our TinyMLOps pipeline.\n",
    "\n",
    "Some of these steps will be constant for almost all pipelines. For instance we will almost always have to optimize our trained model for TinyML with algorithms like quantization or pruning. So we can train our initial model in a way such that the accuracy drop after optimization is reduced. We will look more into these techniques in a bit.\n",
    "\n",
    "Other steps in a machine learning pipeline may be dependent on other factors like client requirements or the choice of hardware. In these cases it is not possible to be able to look-ahead and take into consideration any constraints the system may have when training our model. The easiest way to get around this is to finalize these requirements and decisions before training models.\n",
    "\n",
    "todo\n",
    "\n",
    "You could go the other way around where you choose a hardware based on how big a model is, but this is not always going to be the right decision in the long-run. This is mostly because models are software based and can be updated more easily and frequently (we will talk more about this in a later chapter) as compared to hardware. New model architectures and data preprocessing and optimization techniques are invented every few weeks that will improve the performance of your model. Moreover as you run your system and collect more data, you can retrain your model to improve its performance and eventually train smaller architectures that run more efficiently.\n",
    "\n",
    "However, if you choose a more powerful hardware that consumes more power or has a larger form-factor than the requirements, then updating your hardware can be a very painful process, especially if you already have a fleet of many TinyML devices already deployed. Moreover, to make your system efficient, your software will be tightly coupled with your device and changing it could also result in you having to rethink your model and other software systems as well.\n",
    "\n",
    "## Optimization Look-Ahead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59702969",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
