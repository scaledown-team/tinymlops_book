{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "203ca46a",
   "metadata": {},
   "source": [
    "# Challenges in Training TinyML Models\n",
    "\n",
    "When ML engineers train models, they usually do not think about where the model will be deployed. Their models  are usually deployed in large multi-core systems with support for many instruction sets, compilers and toolkits that can abstract away the complications of the underlying hardware. Moreover, if a model does have poor execution speeds, it is easy to either add more compute resources (like more CPU cores or GPUs) to it or parallelize the execution of the model across multiple devices to improve its latency.\n",
    "\n",
    "However this is not true in the case of TinyML where the final hardware will be resource constrained and have limited instruction set support. Therefore, for TinyML, the final hardware is a bottleneck that we need to keep in mind when training our models. Unfortunately for us, the constraints of the deployment hardware creates a domino effect that affects not only our final model's accuracy, but also the kind of layers we can use and how secure and robust we can make our model.\n",
    "\n",
    "<!--- \n",
    "TODO - Add a sub-section about target hardware: How does target architecture, supported instruction sets etc affect the subsections mentioned below\n",
    "--->\n",
    "\n",
    "## Model Accuracy\n",
    "TinyML devices have less computational power and they do not support many operations. Moreover they do not have a lot of memory. This means that the models we deploy to TinyML devices need to be small, efficient and simple. However these qualities are completely opposite to what makes models accurate. And this is one of the biggest challenges in training TinyML models: How do we make models tiny, while still making sure that they are accurate enough to perform our tasks?\n",
    "\n",
    "## Model Size\n",
    "To execute a neural network in a TinyML device, we need to load the model architecture and weights into memory and also be able to store intermediate activations when executing the model. Since TinyML devices usually have less memory, we need to make sure that the sum of size of the model, its parameters (weights, biases and other internal values) and the largest generated activation can fit in the memory at the same time. While larger model architectures are more accurate, they also have more parameters and generate larger activations. Larger models will  not only take more time to load to memory, but it will also take longer to execute since more operations need to performed. So even if your model can fit into memory, a larger model will have more latency. So when we train our model, we need to make sure that the model size is small such that we can load it into the final hardware we are deploying our model in and its latency is low.\n",
    "\n",
    "## Model Architecture\n",
    "Another factor affecting latency is the architecture of the model. Models with \n",
    "\n",
    "## Model Security and Robustness\n",
    "Finally, many TinyML devices are deployed in remote or open places where they can be affected by environmental factors like sunlight, temperature, dust as well as be prone to physical attacks (for instance to steal model details). For instance, a model that checks for wildlife in a jungle needs to work not only in the morning and night, but also at times with low light, changing shadows, rain, dust and so on. Lately techniques have been developed to train our models such that they are more robust to such environmental changes and to sensor or battery degradation.\n",
    "\n",
    "Since many deployed models may have been trained with sensitive data or may have IP, they can become targets for attackers looking to steal the model architecture and weights. Recent work in side-channel attacks have shown that this is not only possible to be done with high accuracy, but it can be also done quickly and with cheap easily available tools. Thankfully, there are also some ways to keep create models that can make such attacks either difficult or impossible to do.\n",
    "\n",
    "We will talk more about model security and robustness in a later chapter, but now let's take a look at how we can train models for TinyML while solving these challenges."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
